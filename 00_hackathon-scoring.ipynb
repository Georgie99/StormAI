{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring your model against a test set\n",
    "\n",
    "This notebook shows you one way to score your model .pkl aginst a test set using the F1 Score metric. The test set is assumed to have the same file structure as your development set, e.g. Images located in ***testset/pos***, ***testset/neg***, with images in the ***pos*** directory considered labeled to have positive sentiments and the images in the ***neg*** directory labeled to have negative sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastbook import *\n",
    "from fastai.vision.widgets import *\n",
    "from fastai.callback.fp16 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('../models')\n",
    "pickles=path.ls(file_exts='.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#6) [Path('../models/model_resnet34_with8_negfocused.pkl'),Path('../models/model_resnet34_with32_negfocused.pkl'),Path('../models/model_resnet34_with32.pkl'),Path('../models/model_resnet34.pkl'),Path('../models/model_resnet34_with16_negfocused.pkl'),Path('../models/model_resnet34_64.pkl')]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_inf = load_learner(path/pickles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) ['neg','pos']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_inf.dls.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "fname_l=list()\n",
    "pred_l=list()\n",
    "prob_l=list()\n",
    "target_l=list()\n",
    "\n",
    "for lbl in learn_inf.dls.vocab:\n",
    "    target=lbl\n",
    "    path = Path('../testset/'+lbl)\n",
    "    testimgs = path.ls(file_type='image/*')\n",
    "#     pred,pred_idx,probs = learn_inf.predict(testimgs[0])\n",
    "#     print(testimgs[0], \",\", pred, \", %.6f\"% probs[pred_idx])\n",
    "    for img in testimgs:\n",
    "        pred,pred_idx,probs = learn_inf.predict(img);\n",
    "        _,_,_,fname=img.parts\n",
    "        fname_l.append(fname)\n",
    "        pred_l.append(pred)\n",
    "        prob_l.append(probs[pred_idx].item())\n",
    "        target_l.append(target)\n",
    "#         print(fname, \",\", pred, \", %.6f\"% probs[pred_idx].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'image_name':fname_l, 'pred':pred_l, 'prob':prob_l, 'target':target_l}, columns=['image_name','pred','prob','target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of           image_name pred      prob target\n",
       "0   neg_00000005.jpg  neg  0.988190    neg\n",
       "1   neg_00000006.jpg  neg  0.998447    neg\n",
       "2  neg_00000007.jpeg  neg  0.999457    neg\n",
       "3   neg_00000004.jpg  neg  0.959892    neg\n",
       "4  neg_00000002.jpeg  neg  0.984903    neg\n",
       "5   pos_00000006.jpg  pos  0.999714    pos\n",
       "6   pos_00000007.jpg  pos  0.995844    pos\n",
       "7   pos_00000002.jpg  pos  0.997900    pos\n",
       "8   pos_00000004.jpg  pos  0.999293    pos\n",
       "9   pos_00000005.jpg  pos  0.999506    pos>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('../testset')\n",
    "df.to_csv(path/'submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('../testset')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score for only entire test set\n",
    "\n",
    "f1_score(df.target, df.pred, labels=learn_inf.dls.vocab, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/fastai2/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1465: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score for only target=pos\n",
    "\n",
    "f1_score(df[df['target'] == 'pos'].target, df[df['target'] == 'pos'].pred, labels=learn_inf.dls.vocab, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score for only target=neg\n",
    "\n",
    "f1_score(df[df['target'] == 'neg'].target, df[df['target'] == 'neg'].pred, labels=learn_inf.dls.vocab, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "fastai2",
   "language": "python",
   "name": "conda-env-fastai2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
